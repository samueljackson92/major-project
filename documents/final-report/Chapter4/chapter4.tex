\chapter{Critical Evaluation}
This final chapter presents my evaluation of the project as a whole, outlines directions for future work in this area and provides a personal reflection on how I have handled and executed this project.

\section{Evaluation of the Project}
Overall this project has managed to answer some of the original questions laid out during the problem analysis and I feel that while the results of this project are not necessarily  as good as I was hoping for this has still been a fun project and excellent learning experience. I started this project with very minimal knowledge of image processing and analysis techniques (other than what had been taught in other university modules) and no knowledge of dimensionality reduction algorithms. I also started with module with very little prior knowledge linear algebra, something which I would call a prerequisite for properly understanding many dimensionality reduction algorithms.

There were several goals that I set out to achieve as part of this project. This section will discuss them one by one and provide a review of what was achieved, what went well, and what went wrong.

The first goal was to image features from both real and synthetic mammogram images based on a variety of common techniques used with mammographic images. This required an extensive review of existing literature on the subject and the selection of a few techniques from each case to be used as part of the final system pipeline.

I feel that the features chosen for use in this project were entirely appropriate. I feel that starting with shape based features was a good idea and that the two techniques chosen produced reasonable results in practice. Blob features proved more complicated to implement than line features overall, but blob features end up producing better results and so I felt the extra effort involved was justified. 

After extracting shape features I wanted to also try and focus on extracting other types of feature from the images. An difficult issue with this the time complexity required to extract more complicated features (i.e. texture features) from an image. For this reason I chose to use the patch of image defined by shape features as way narrow down the amount of processing required on each image. The reasoning behind this was that because a ROI has already been defined, extracting texture and intensity features from that region might yield additional useful information. 

My biggest regret with feature extraction is that I didn't realise until after the extracting texture and intensity features that the intensity distribution between real and synthetic images is very different, leading to the clear separation between the two datasets that can be viewed in all of the visualisations of these spaces. While this proved to be a disappointing result it does correlate with the some of the limitations proposed by the authors of the synthetic dataset in refs. \cite{bakic2002mammogram1, bakic2002mammogram2, bakic2003mammogram3}. In terms of the shape features, the synthetic mammograms at least appeared to be in the same space as their real counterparts however but the extracted features were still unable to properly align the synthetics under projection according to their associated level of risk.

The second goal was to perform dimensionality reduction on the feature space generated by each of these techniques to produce two and three dimensional visual representations. A variety of different techniques were investigated as part of the initial stage of this project and a few fundamentally different approaches were selected to be used in the results of this project.

As I have briefly mentioning in the first paragraph of this section, one of the major challenges that I felt I faced with this project was trying fully understand the core principles and mathematics behind how dimensionality reduction algorithms worked. This was an area in which I had zero knowledge of before I began this project and subsequently I spent a significant proportion of the early part of the project trying researching how some of the more common techniques work along with the strengths and weaknesses of each. 

There was very little technical effort involved with this component of the project. All of the techniques used in the project were already implemented as part of the scikit-learn python library. I felt that there was no need to reimplement complicated dimensionality reduction algorithms when they were already readily available in a well known and well tested package. I feel that the choice of dimensionality reduction techniques used was justified and that a small yet diverse mix of approaches were presented along with the results.

The third goal was to produce a way of evaluating the quality of the visualisations produced by dimensionality reduction. Once again this was investigated as part of the initial stage of the project and quality metrics derived from co-ranking matrices were eventually selected as the best choice due to the large number of measures that can be derived from them.

Again, like dimensionality reduction, this was an area that required more background reading that actual implementation. The implementation of the co-ranking matrix and the measures derived from it are actually only a few hundred lines of code, but understanding how the co-ranking matrix is derived and what the metrics subsequently derived from it measure took the majority of the effort. I felt that this was a justified approach to assess the quality of the mappings produced but that there was much more that could have been done in this area, for example in visualising the quality of the mapping in a similar way as suggested by Mokbel et al \cite{mokbel2013visualizing}.

The third aim was to integrate these separate components into an image analysis pipeline. This goal was achieved as part of a Python image analysis package produced during this project. While the implementation offered in this package is far from perfect I feel that it provides a good basic implementation of a image analysis pipeline. I feel that the most well developed part of the system is the image processing component. This provides reusable way to implement new feature detection routines through an interface that provides support for multiprocessing on a per image basis.

There are, however, some parts of the system I am not entirely happy with. The most notable of these is the section analysis module. This has mostly been developed on an ad-hoc basis depending on the functions I required to perform analysis. This module has ended up being a collection of the more common operations used when processing the features using IPython notebooks. It as particularly difficult deciding which functions to include in this module. The problem is that the operations you need to perform on the data largely depends on the patterns you see in the data itself. On one hand obviously not every single thing should be included, but on the other the aim of good software is to produce reusable components. Based on this I chose to work on the code in an IPython notebook and if I required the function in more than one place, move it into the analysis module.

The final aim of this project was to try and examine the difference between projections of the feature space of real and synthetic mammogram datasets. In this goal I feel that I have only been partially successful. While I feel that the results of the experiments performed allow us to draw some valid conclusions about the the differences between the two datasets I also feel that I could have spent more time constructing system to evaluate the results.

I feel that by far my most major weakness in the project was failing to full develop how to formally compare the feature spaces generated by the two datasets. Throughout the project my analysis of the dimensionality reduction was mostly based on visual examination of the mapping and trying to relate back to the original data. Looking at how images change across the lower dimensional embedding and trying to relate this to why the synthetics images show up as different works fine, but does not provide quantitive evaluation of how the feature spaces overlap.

I believe that this weakness was mostly caused by my own failure to plan this section of the project correctly ahead of time. This was the first research style project I have undertaken and in reflection I should have spent more time planning how to formally compare the two datasets. In reflection I feel that I perhaps jumped into development and implementation too soon without giving enough thought as to how to formally evaluate my results. 

In reflation on the choice of language used I feel that Python was an excellent choice for this project. The fact that Python is a very high level language but with the capability of producing decent performance when needed. The scipy and numpy libraries in particular saved me from having to reinvent the wheel on a lot of things. The general design of the language allowed me to work at a higher level compared to other languages. I found that I was easily able to focus on actually implementing what was required for the project than spending my time writing boilerplate code. Python's C API also came in handy as predicted. My implementation of the deformable convolution would have been much, much slower if it had been implemented in pure Python code.

My approach to the management of the project seemed to work reasonably well. The agile based process I used in this project allowed me all of the flexibility I needed to work on what needed to be done rather than focussing on rigid requirements. As predicted I found that it really helped to be adaptive the project as new issues and challenges arose.

One thing that I found worked particularly well was the having a weekly iteration which was timed to start and finish with my weekly dissertation meeting. The discussion in these meetings allowed me to decide what ``stories" should be worked on throughout the week. During the first few iterations I tended to find that I would be overly ambitious about the number of task I could complete in a week. In the later iterations I found that this started to balance out more as I became more aware of how much I could feasibly achieve in a week.

I found the use of test driven development to be difficult to adhere to at times. I also often found it challenging to write decent tests for the software I was producing. Many of the functions in the package either return output that is difficult to evaluate for correctness. Another issue with testing was the fact that a number routines take a quite a long time to execute (i.e. greater than a minute per image). As this execution time makes it infeasible for unit testing I found that regression testing of the larger components of the system often proved to be more useful than the smaller unit tests. I did, however, find that both types of testing used in the project were extremely useful debugging the program, especially in the later stages of development as the system grew larger. 

I also had a positive experience with using continuous integration throughout my project. My primary workflow involved doing most of my work on separate git branches and merging the finished work into the main branch upon completion. I found that it provided a useful safety net for checking if I broke something while working on code in two different branches in parallel.

In conclusion I found this project to be a mostly enjoyable and rewarding experience. I found the most challenging and rewarding aspect to be trying to understand the concepts that I have been trying to make use. I strongly feel that while many of aspects of this project weren't a success I have learned a great deal how to better structure a research based project like this. I have also been left with a much stronger knowledge and personal interest in the image analysis and dimensionality reduction domains.


\section{Future Work}
\label{sec:future-work}
There are a multitude of ways in which this project could be extended. This section lists of of the ways in which I believe the project could be expanded along with a justification as to why each point would be worth investigating.

One piece of additional work I would have liked to of investigated is to run the projections produced through a couple of common classifiers (such as, for example, a non-linear SVM). Alongside the quality metrics for visualisation, this would give me some raw numbers quantifying how well a classifier performs on the feature spaces produced by my implementations. While this would have little benefit to the projects main goals of examining the differences between synthetic and real feature spaces, it would have given me . This would have the additional advantage of providing a quantitive way of adjusting the parameters for the feature extraction and dimensionality reduction components of the system.

Another improvement I would have liked to have made to the system is the to speed up the performance of the deformable convolution module. Currently this module provides a very slow implementation of the convolution operator. While this is necessarily slow around the edge of mask being convolved with the image because of the nature of modifying the kernel with every convolution this does not need to be the case across the main portion of the image. For most of the breast two 1D second derivative Gaussian kernel in vertical and horizontal directions could be used and simply summed together to produce the same response but reducing the overall complexity.

I believe that the best feature used in this project was the multi-scale blob feature. I think that if I were to continue developing this project I would have experimented with making the linear structure feature multi-scale. I think that this feature misses some of the very small and very large scale linear structures because the algorithm has trouble detecting features across all scales with the same parameters (bins size, threshold etc.). For smaller lines the kernel proves to be too large and the line response does not show through strongly enough to avoid getting removed by thresholding. Likewise for larger lines the opposite is true; the response from the kernel in all directions appears uniform. Using this, duplicated lines detected across scales could be removed using a merging method like blob features, but it would obviously have to be more complicated because of the inconsistent size and shape of line features.

With regards to the visualisation aspect of this project I feel that there are several areas for further expansion. One method that captures my attention was a point wise measure of quality for lower dimensional mappings derived from the co-ranking matrix \cite{mokbel2013visualizing}. This can be used to colour the data points in a scatterplot of the lower dimensional mapping of a feature space. I feel that an approach based on a technique such as this provide a much easier visual interpretation of the quality of the visualisation in comparison of the measures of trustworthiness, continuity, and LCMC.

During this project I have not focussed too much on the use of higher dimensional representations of the feature space. While these are often not as easy to interpret as a 2D or 3D scatterplot, they can provide useful additional insight. One thing which I would have liked to have tried in my project is to implement a method of ordering features according to there importance such as proposed by many methods listed in ref. \cite{bertini2011quality}. In particular my imagination was captured by the discussion of ordering parallel coordinates axes using the Hough transform \cite{tatu2009combining} and interactive feature selection by information loss \cite{johansson2009interactive}.

One aspect that I did not focus on much during the course of this project was ``topological" aspect in the project title. Mid way through the course of this project I began to read into the field of topological data analysis \cite{carlsson2009topology}. I think while this area is not necessarily directly related to the examining the goals of this specific project it would be an area I feel would be worth examining. I think that exploring the topology of the mammographic feature space might yield some interesting relationships and I would be excited to explore this area further.

 